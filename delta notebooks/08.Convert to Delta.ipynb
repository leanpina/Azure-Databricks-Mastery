{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c37d6f-d9f3-4c78-8d88-afc640724f28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.deltadbstgpina.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.deltadbstgpina.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.deltadbstgpina.dfs.core.windows.net\", \"39113859-cd89-41ee-af19-bedb601eac67\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.deltadbstgpina.dfs.core.windows.net\", \"U6D8Q~EI-8UPcYENtNQ9GFpKRqS~81_U8oBzXcNX\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.deltadbstgpina.dfs.core.windows.net\", \"https://login.microsoftonline.com/63c8a3d9-530b-4409-afe8-69c29700ddff/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a2f1be1-fd32-4631-8916-5fbee0fe1983",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source = 'abfss://test@deltadbstgpina.dfs.core.windows.net/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "441b9f96-efc3-46ab-888e-e62e41590756",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Reading data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b167c71-6d4d-4320-8ac8-223c10fe0606",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,DateType,FloatType,DoubleType\n",
    "\n",
    "schema1 = StructType([\n",
    "    StructField('Education_Level',StringType()),\n",
    "    StructField('Line_Number',IntegerType()),\n",
    "    StructField('Employed',IntegerType()),\n",
    "    StructField('Unemployed',IntegerType()),\n",
    "    StructField('Industry',StringType()),\n",
    "    StructField('Gender',StringType()),\n",
    "    StructField('Date_Inserted',StringType()),\n",
    "    StructField('dense_rank',IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f43af25-70e7-4e1d-a836-dc94f2eb29fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read.format('csv')\n",
    "            .option('header','true')\n",
    "            .schema(schema1)\n",
    "            .load(f'{source}/files/*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "121db80d-5c37-439b-8bdc-fa758e2c268f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "O comando abaixo transforma o dataframe em arquivo parquet dentro do folder \"OnlyParquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edef071e-4c02-428c-a351-50adbf011fdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('parquet').save(f'{source}/OnlyParquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc8ab2bf-f1bb-430c-8ed4-4e1ff0215b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Agora fazemos uma transformação do nome de uma coluna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3449875c-3e50-4a97-8b5c-667a88862474",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfnew = df.withColumnRenamed(existing='Line_Number',new='LNo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "023dc637-3a04-4e04-b4bd-202c7102a021",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Overwriting Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18aa2b2d-b268-4eef-8bb1-d7662d66124e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfnew.write.format('parquet').mode('overwrite').save(f'{source}/OnlyParquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe128f74-3fa8-4a88-9fad-b76761615ea9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Reading overwritten parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14cc2d26-f12c-4e44-ac36-ae28f76eae84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ov = (spark.read.format('parquet')\n",
    "            .load(f'{source}/OnlyParquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5b3581-c280-4295-8c3a-6b20fcff9e35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_ov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03dc325c-b4b2-4d45-8166-8d096ef524e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Convert to Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed32daeb-590d-4767-96be-b2c236ff625b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Compreendendo a Conversão Delta\n",
    "\n",
    "O conceito de \"Conversão Delta\" não é explicitamente mencionado nas fontes fornecidas. No entanto, as fontes discutem extensivamente o Delta Lake, um framework de armazenamento de código aberto que oferece confiabilidade e recursos transacionais a data lakes. Um aspecto crucial abordado é a capacidade de converter pastas Parquet existentes em Delta Lake, o que pode ser interpretado como \"Conversão Delta\" no contexto fornecido. \n",
    "\n",
    "**Aqui está um resumo dos principais pontos sobre a conversão de Parquet para Delta Lake, com base nas fontes:**\n",
    "\n",
    "* **Cenário:** Imagine trabalhar em um projeto que usa arquivos Parquet, um formato popular para projetos em tempo real devido à sua compressão e suporte a esquema. Um novo requisito surge para converter esses arquivos Parquet para Delta Lake para aproveitar os recursos do Delta Lake.\n",
    "* **Processo de Conversão:** O processo é direto. Em vez de gravar em um novo local, o comando SQL `CONVERT TO DELTA` é usado para converter o local existente para Delta Lake. Você precisa especificar o formato do arquivo (neste caso, Parquet) e o caminho da pasta.\n",
    "* **Impacto da Conversão:** Após a conversão, um diretório `_delta_log` aparece na pasta, sinalizando a presença do Delta Lake. Esse log de transações é fundamental para os recursos do Delta Lake.\n",
    "* **Benefícios:** A conversão permite aproveitar os recursos do Delta Lake, incluindo histórico de versão e reversão para estados anteriores de dados. O log de transações registra cada operação, permitindo auditoria e análise de alterações de dados.\n",
    "* **Limitações:** O log de transações rastreia operações apenas após a conversão para Delta Lake. As transformações aplicadas aos arquivos Parquet antes da conversão não são registradas no histórico de versões do Delta Lake.\n",
    "\n",
    "As fontes fornecem um guia passo a passo sobre como executar essa conversão e destacam seus benefícios e limitações. É essencial entender que, embora o termo \"Conversão Delta\" não seja usado diretamente, o processo descrito de conversão de Parquet para Delta Lake se alinha com o conceito e fornece uma solução prática para integrar arquivos Parquet existentes em um ambiente Delta Lake. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7088b672-e959-4319-a61d-c491bc36edaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CONVERT TO DELTA parquet.`abfss://test@deltadbstg.dfs.core.windows.net/OnlyParquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "655f9913-5020-4630-a5d5-1efb98b81865",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Somente temos o log de transações apartir do momento da conversão para Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70dd8aed-1b61-4bdb-a101-b0f90bffcf38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY delta.`abfss://test@deltadbstg.dfs.core.windows.net/OnlyParquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09b52c11-17e5-485f-9512-8b4a434d8af9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 321285118064569,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "08.Convert to Delta",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
