{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "348b0a88-c8fc-4d15-87e0-58f9cc78eb91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Usamos o delta file com acesso ao datalake pelo processo de autenticação do OAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af64981-ffdd-4cdd-9a70-5fc385ac8677",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.deltadbstgpina.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.deltadbstgpina.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.deltadbstgpina.dfs.core.windows.net\", \"39113859-cd89-41ee-af19-bedb601eac67\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.deltadbstgpina.dfs.core.windows.net\", \"U6D8Q~EI-8UPcYENtNQ9GFpKRqS~81_U8oBzXcNX\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.deltadbstgpina.dfs.core.windows.net\", \"https://login.microsoftonline.com/63c8a3d9-530b-4409-afe8-69c29700ddff/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79ff7eb-2807-4658-999f-3dbb120fd8d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source = \"abfss://test@deltadbstgpina.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b56350a-5220-4422-885a-3e2f1bb06870",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Reading the parquet file into dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af783b77-cf7d-474f-89b6-69b13d4625c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Usaremos o folder \"test / ParquetFolder\" devido ao fato do conteúdo não ser do tipo \"`delta`\"....Sabemos disso devido a ausência do folder \"`_delta_log`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1699f855-a5d7-4b2d-8ecc-f4f7206a3fea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read.format('parquet')\n",
    "                .load(f'{source}/ParquetFolder/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a775b6-6962-43f3-b497-036f622db926",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0192a0db-7c65-4ba2-bae1-b9dec140ab41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dentro do schema \"delta\" será criado um table \"deltaspark\".\\\n",
    "Toda vez que se cria uma tabela no hive_metastore sem definir uma localização externa, tal tabela será do tipo \"MANAGED\", ou seja será uma tabela gerenciada pelo databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50e7eb1-6cb3-4fb3-b7dc-f819856e25b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df.write.format('delta')\n",
    "        .mode('overwrite')\n",
    "        .saveAsTable('`delta`.DeltaSpark'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b8b1013-1d89-4527-9d83-600066a77b62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para a localização da tabela em 'dbfs:/user/hive/warehouse/delta.db/deltaspark', pode-se verificar a geração do folder _delta_log e de um parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0824601-a532-4f47-8425-1d1d40bb5c99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('dbfs:/user/hive/warehouse/delta.db/deltaspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020be199-b228-4d5c-b8ba-5ffc5fda26c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('dbfs:/user/hive/warehouse/delta.db/deltaspark/_delta_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dac6671-080a-4439-a93f-041734f81513",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.format('text').load('dbfs:/user/hive/warehouse/delta.db/deltaspark/_delta_log/00000000000000000000.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67238497-850b-4671-8203-5afd87f75cce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.format('delta').load('dbfs:/user/hive/warehouse/delta.db/deltaspark'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66d7ae46-ef3d-42db-9184-72be21ed436d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04.Creating delta tables using PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
